{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef358b5-3493-4525-8e45-378219887a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.metrics import AUC\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768ac0a2-5718-4c9c-8945-71ceb0dd249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e08dec9-e0ba-4ede-b200-c02ab92ce5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Set file paths to image files\n",
    "#project_path = \"C:/Users/Zeel soni/Downloads/archive\"\n",
    "train_path = \"C:/Users/Zeel soni/Downloads/archive/chest_xray/train\"\n",
    "val_path =  \"C:/Users/Zeel soni/Downloads/archive/chest_xray/val\"\n",
    "test_path = \"C:/Users/Zeel soni/Downloads/archive/chest_xray/test\"\n",
    "\n",
    "## Set up hyperparameters that will be used later\n",
    "hyper_dimension = 64\n",
    "hyper_batch_size = 128\n",
    "hyper_epochs = 100\n",
    "hyper_channels = 1\n",
    "hyper_mode = 'grayscale'\n",
    "\n",
    "## Generate batches of image data (train, validation, and test) with data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, \n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2, \n",
    "                                   horizontal_flip = True)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory = train_path, \n",
    "                                                    target_size = (hyper_dimension, hyper_dimension),\n",
    "                                                    batch_size = hyper_batch_size, \n",
    "                                                    color_mode = hyper_mode,\n",
    "                                                    class_mode = 'binary', \n",
    "                                                    seed = 42)\n",
    "val_generator = val_datagen.flow_from_directory(directory = val_path, \n",
    "                                                 target_size = (hyper_dimension, hyper_dimension),\n",
    "                                                 batch_size = hyper_batch_size, \n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = hyper_mode,\n",
    "                                                 shuffle=False,\n",
    "                                                 seed = 42)\n",
    "test_generator = test_datagen.flow_from_directory(directory = test_path, \n",
    "                                                 target_size = (hyper_dimension, hyper_dimension),\n",
    "                                                 batch_size = hyper_batch_size, \n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = hyper_mode,\n",
    "                                                 shuffle=False,\n",
    "                                                 seed = 42)\n",
    "\n",
    "test_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdf7387-2bbb-446c-a964-e4f9ca56ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zeel soni\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zeel soni\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3s/step - auc: 0.4986 - loss: 0.6629 - val_auc: 0.3359 - val_loss: 0.9819\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - auc: 0.4293 - loss: 0.6643 - val_auc: 0.3984 - val_loss: 0.7320\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - auc: 0.5163 - loss: 0.5949 - val_auc: 0.3672 - val_loss: 0.8388\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - auc: 0.5112 - loss: 0.5849 - val_auc: 0.3828 - val_loss: 0.8328\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - auc: 0.5746 - loss: 0.5924 - val_auc: 0.5547 - val_loss: 0.7386\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - auc: 0.6518 - loss: 0.5259 - val_auc: 0.5547 - val_loss: 0.8755\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - auc: 0.7377 - loss: 0.5428 - val_auc: 0.7734 - val_loss: 0.6971\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234ms/step - auc: 0.8603 - loss: 0.5009 - val_auc: 0.7812 - val_loss: 0.6578\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - auc: 0.9247 - loss: 0.4425 - val_auc: 0.7500 - val_loss: 0.7255\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - auc: 0.8880 - loss: 0.4334 - val_auc: 0.7734 - val_loss: 0.6889\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - auc: 0.9026 - loss: 0.3940 - val_auc: 0.8125 - val_loss: 0.5790\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - auc: 0.9298 - loss: 0.3003 - val_auc: 0.8203 - val_loss: 0.7597\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - auc: 0.9160 - loss: 0.3289 - val_auc: 0.8125 - val_loss: 1.0057\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - auc: 0.9108 - loss: 0.3678 - val_auc: 0.8750 - val_loss: 0.6377\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - auc: 0.9204 - loss: 0.3379 - val_auc: 0.8750 - val_loss: 0.4584\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - auc: 0.9387 - loss: 0.3021 - val_auc: 0.8828 - val_loss: 0.5486\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - auc: 0.8993 - loss: 0.3513 - val_auc: 0.8906 - val_loss: 0.4427\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - auc: 0.9511 - loss: 0.2572 - val_auc: 0.8984 - val_loss: 0.4343\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - auc: 0.9547 - loss: 0.2517 - val_auc: 0.8750 - val_loss: 0.5770\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3s/step - auc: 0.9301 - loss: 0.2895 - val_auc: 0.8906 - val_loss: 0.5274\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))#Input Layer\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))#Hidden Layers\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))#Hidden Layers\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))#Hidden Layers\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(activation='relu', units=128))\n",
    "cnn.add(Dense(activation='sigmoid', units=1))#Output Layer\n",
    "\n",
    "cnn.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=[AUC()])\n",
    "cnn_model = cnn.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=5,  # Reduce for debugging\n",
    "    epochs=20,           # Fewer epochs for debugging\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=2, # Reduce for debugging\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a72ad4-16f7-482a-b2fa-a3e19a2a35e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Train & validation loss\n",
    "train_loss = cnn_model.history['loss']\n",
    "val_loss = cnn_model.history['val_loss']\n",
    "\n",
    "# Train & validation AUC\n",
    "train_auc = cnn_model.history[list(cnn_model.history.keys())[3]]\n",
    "val_auc = cnn_model.history[list(cnn_model.history.keys())[1]]\n",
    "\n",
    "# True labels and predictions\n",
    "y_true = test_generator.classes\n",
    "Y_pred = cnn.predict(test_generator, steps=len(test_generator))\n",
    "y_pred = (Y_pred > 0.5).flatten()\n",
    "y_pred_prob = Y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd26f5b4-2005-45e6-9683-53413605b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Summary Statistics]\n",
      "Accuracy = 86.38% | Precision = 86.75% | Recall = 92.31% | Specificity = 76.50% | F1 Score = 89.44%\n"
     ]
    }
   ],
   "source": [
    "## Summary Statistics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "accuracy = (TP + TN) / np.sum(cm)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "# Print Summary\n",
    "print(f'[Summary Statistics]\\n'\n",
    "      f'Accuracy = {accuracy:.2%} | Precision = {precision:.2%} | '\n",
    "      f'Recall = {recall:.2%} | Specificity = {specificity:.2%} | '\n",
    "      f'F1 Score = {f1:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
